@misc{inspect2024,
  title = {Inspect AI: A Framework for Large Language Model Evaluations},
  author = {{UK AI Safety Institute}},
  year = {2024},
  howpublished = {\url{https://github.com/UKGovernmentBEIS/inspect_ai}},
  note = {Accessed: 2026-01-29}
}

@misc{inspectevals2024,
  title = {Inspect Evals: Pre-built Evaluation Suite},
  author = {{UK AI Safety Institute}},
  year = {2024},
  howpublished = {\url{https://github.com/UKGovernmentBEIS/inspect_evals}},
  note = {Accessed: 2026-01-29}
}

@misc{controlarena2024,
  title = {ControlArena: Evaluating AI Control Protocols},
  author = {{UK AI Safety Institute}},
  year = {2024},
  howpublished = {\url{https://github.com/UKGovernmentBEIS/control-arena}},
  note = {Accessed: 2026-01-29}
}

@misc{sail2024,
  title = {SAIL Framework: Secure AI Lifecycle},
  author = {{Pillar Security}},
  year = {2024},
  howpublished = {\url{https://www.pillar.security/sail}},
  note = {Accessed: 2026-02-06}
}

@misc{transformerlens2024,
  title = {TransformerLens: A Library for Mechanistic Interpretability of GPT-Style Language Models},
  author = {Nanda, Neel and others},
  year = {2024},
  howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
  note = {Accessed: 2026-01-29}
}

@misc{circuittracer2025,
  title = {circuit-tracer: Finding Circuits Using Transcoder Features},
  author = {Hanna, Michael and Piotrowski, Mateusz and Lindsey, Jack and Ameisen, Emmanuel},
  year = {2025},
  howpublished = {\url{https://github.com/safety-research/circuit-tracer}},
  note = {The first two authors contributed equally and are listed alphabetically. Accessed: 2026-01-29}
}

@misc{nnsight2024,
  title = {nnsight: A Framework for Interpreting PyTorch Models},
  author = {{NDIF Team}},
  year = {2024},
  howpublished = {\url{https://github.com/ndif-team/nnsight}},
  note = {Accessed: 2026-01-29}
}

@misc{saelens2024,
  title = {SAELens: Training Sparse Autoencoders on Language Models},
  author = {Bloom, Joseph and Tigges, Curt and Duong, Anthony and Chanin, David},
  year = {2024},
  howpublished = {\url{https://github.com/jbloomAus/SAELens}},
  note = {Accessed: 2026-01-29}
}

@misc{neuronpedia2024,
  title = {Neuronpedia: Platform for Sharing Interpretability Research},
  author = {Lin, Johnny},
  year = {2024},
  howpublished = {\url{https://neuronpedia.org}},
  note = {Accessed: 2026-01-29}
}

@misc{circuitsvis2024,
  title = {CircuitsVis: Mechanistic Interpretability Visualizations using React},
  author = {{TransformerLens Organization}},
  year = {2024},
  howpublished = {\url{https://github.com/TransformerLensOrg/CircuitsVis}},
  note = {Accessed: 2026-01-29}
}

@misc{mossing2024transformer,
  title = {Transformer Debugger},
  author = {Mossing, Dan and Bills, Steven and Tillman, Henk and Dupré la Tour, Tom and Cammarata, Nick and Gao, Leo and Achiam, Joshua and Yeh, Catherine and Leike, Jan and Wu, Jeff and Saunders, William},
  year = {2024},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/openai/transformer-debugger}},
  note = {Accessed: 2026-01-29}
}

@misc{prisma2025,
      title={Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video}, 
      author={Sonia Joseph and Praneet Suresh and Lorenz Hufe and Edward Stevinson and Robert Graham and Yash Vadi and Danilo Bzdok and Sebastian Lapuschkin and Lee Sharkey and Blake Aaron Richards},
      year={2025},
      eprint={2504.19475},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.19475}, 
}

@inproceedings{wu2024pyvene,
  title = {pyvene: A Library for Understanding and Improving {P}y{T}orch Models via Interventions},
  author = {Wu, Zhengxuan and Geiger, Atticus and Arora, Aryaman and Huang, Jing and Wang, Zheng and Goodman, Noah and Manning, Christopher and Potts, Christopher},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)},
  month = jun,
  year = {2024},
  address = {Mexico City, Mexico},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2024.naacl-demo.16},
  pages = {158--165}
}

@article{cunningham2023sparse,
  title = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal = {arXiv preprint arXiv:2309.08600},
  year = {2023}
}

@article{olah2020zoom,
  title = {Zoom In: An Introduction to Circuits},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal = {Distill},
  year = {2020},
  url = {https://distill.pub/2020/circuits/zoom-in/}
}

@article{amodei2016concrete,
  title = {Concrete Problems in AI Safety},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
  journal = {arXiv preprint arXiv:1606.06565},
  year = {2016}
}

@misc{bau2022baukit,
  title = {BauKit: Tools for Analyzing Neural Network Internals},
  author = {Bau, David},
  year = {2022},
  howpublished = {\url{https://github.com/davidbau/baukit}},
  note = {Accessed: 2026-01-29}
}

@misc{concordance2024,
  title = {Concordance Token Injection Lab},
  author = {{Concordance Research}},
  year = {2024},
  howpublished = {\url{https://research.concordance.co/playground}},
  note = {Accessed: 2026-02-06}
}

@article{pyreft2024,
  title={{ReFT}: Representation Finetuning for Language Models},
  author={Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D. and Potts, Christopher},
  booktitle={arXiv:2404.03592},
  url={arxiv.org/abs/2404.03592},
  year={2024}
}

@misc{openmoss2024,
  title = {Language-Model-SAEs: Performant Framework for Training and Analyzing Sparse Autoencoders},
  author = {Ge, Xuyang and Shu, Wentao and Wang, Junxuan and Zhou, Guancheng and Wu, Jiaxing and Zhu, Fukang and Chen, Lingjie and He, Zhengfu},
  year = {2024},
  howpublished = {\url{https://github.com/OpenMOSS/Language-Model-SAEs}},
  note = {Accessed: 2026-01-29}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR},
  url={https://proceedings.mlr.press/v202/biderman23a.html}
}

@article{groeneveld2024olmo,
  title={{OLMo}: Accelerating the Science of Language Models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  journal={arXiv preprint arXiv:2402.00838},
  year={2024},
  url={https://arxiv.org/abs/2402.00838}
}

@article{liu2023llm360,
  title={{LLM360}: Towards Fully Transparent Open-Source {LLMs}},
  author={Liu, Zhengzhong and Qiao, Aurick and Neiswanger, Willie and Wang, Hongyi and Tan, Bowen and Tao, Tianhua and Li, Junbo and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and others},
  journal={arXiv preprint arXiv:2312.06550},
  year={2023},
  url={https://arxiv.org/abs/2312.06550}
}

@article{lieberum2024gemma,
  title={Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2},
  author={Lieberum, Tom and Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Sonnerat, Nicolas and Varma, Vikrant and Kramar, Janos and Dragan, Anca and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2408.05147},
  year={2024},
  url={https://arxiv.org/abs/2408.05147}
}

@misc{somvanshi2025bridging,
  title={Bridging the Black Box: A Survey on Mechanistic Interpretability in AI},
  author={Somvanshi, Shriyank and Islam, Md Monzurul and Rafe, Amir and Tusti, Anannya Ghosh and Chakraborty, Arka and Baitullah, Anika and Chowdhury, Tausif Islam and Alnawmasi, Nawaf and Dutta, Anandi and Das, Subasish},
  year={2025},
  howpublished={SSRN},
  url={https://papers.ssrn.com/abstract=5345552},
  note={Accessed: 2026-02-06}
}

@article{rai2024practical,
  title={A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
  author={Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  journal={arXiv preprint arXiv:2407.02646},
  year={2024},
  url={https://arxiv.org/abs/2407.02646}
}

@misc{arora2024causalgym,
  title={CausalGym: Benchmarking causal interpretability methods on linguistic tasks},
  author={Arora, Aryaman and others},
  year={2024},
  note={Referenced in Rai et al., 2024}
}

@misc{huang2024ravel,
  title={RAVEL: Resolving Attribute--Value Entanglements in Language Models},
  author={Huang, Jing and others},
  year={2024},
  note={Referenced in Rai et al., 2024}
}

@misc{mueller2025mib,
  title={Mechanistic Interpretability Benchmark},
  author={Mueller, Aaron and others},
  year={2025},
  note={Referenced in Rai et al., 2024}
}

@misc{karvonen2025saebench,
  title={SAEBench: A Comprehensive Evaluation Suite for Sparse Autoencoders},
  author={Karvonen, Joonas and others},
  year={2025},
  note={Referenced in Rai et al., 2024}
}

@misc{schwettmann2023find,
  title={FIND: Function INterpretation and Description},
  author={Schwettmann, Sarah and others},
  year={2023},
  note={Referenced in Rai et al., 2024}
}

@misc{lindner2024synthetic,
  title={Synthetic Transformers with Known Circuits},
  author={Lindner, David and others},
  year={2024},
  note={Referenced in Rai et al., 2024}
}

@misc{gupta2024semisynthetic,
  title={Semi-Synthetic Transformers for Circuit Evaluation},
  author={Gupta, Tanishq and others},
  year={2024},
  note={Referenced in Rai et al., 2024}
}
