\documentclass[aspectratio=169]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{natbib}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Define Colors
\definecolor{primary}{HTML}{7B4384}
\definecolor{secondary}{HTML}{A577B8}
\definecolor{tertiary}{HTML}{765F93}
\definecolor{accent}{HTML}{DDAEE4}

% Beamer Theme Configuration
\usetheme{Madrid}
\usecolortheme[named=primary]{structure}

\setbeamercolor{palette primary}{bg=primary,fg=white}
\setbeamercolor{palette secondary}{bg=secondary,fg=white}
\setbeamercolor{palette tertiary}{bg=tertiary,fg=white}
\setbeamercolor{palette quaternary}{bg=accent,fg=black}
\setbeamercolor{titlelike}{parent=palette primary}
\setbeamercolor{block title}{bg=primary,fg=white}
\setbeamercolor{block body}{bg=accent!20!white,fg=black}
\setbeamercolor{block title example}{bg=secondary,fg=white}
\setbeamercolor{block body example}{bg=accent!20!white,fg=black}
\setbeamercolor{block title alerted}{bg=tertiary,fg=white}
\setbeamercolor{block body alerted}{bg=accent!20!white,fg=black}
\setbeamercolor{alerted text}{fg=primary}
\setbeamercolor{structure}{fg=primary}

% Title Information
\title{The Technical Challenge of AI Alignment}
\subtitle{AI Safety Fundamentals}
\author{Elisabetta Rocchetti}
\date{\today}
\institute{AI Safety Course}

\begin{document}

% Title Slide
\begin{frame}
    \titlepage
\end{frame}

% Table of Contents
\begin{frame}{Outline}
    \tableofcontents
\end{frame}

% Section 1
\section{Introduction: How Modern AI Works}

\begin{frame}{From Programming to Growing AI Systems}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Traditional Programming}
            \begin{itemize}
                \item Explicit instructions
                \item Deterministic behavior
                \item Fully understood logic
            \end{itemize}
            
            \vspace{1em}
            
            \textbf{Modern Deep Learning}
            \begin{itemize}
                \item ``Growing'' systems through training
                \item Emergent behavior
                \item Opaque internal mechanisms
            \end{itemize}
            
            \vspace{0.5em}
            \tiny~\citep{Deng2018Artificial}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                % PLACEHOLDER: Diagram showing traditional programming (flowchart with explicit steps) 
                % vs. deep learning (neural network with data flowing through layers)
                \includegraphics[width=\textwidth]{imgs/programming-vs-ml.jpeg}
                \caption{Programming vs. Training}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{The Training Process}
    \begin{block}{Stochastic Gradient Descent}
        Iteratively adjusts model parameters to maximize performance on a training objective~\citep{amari1993backpropagation, bai2022training}
    \end{block}

    \vspace{0.5em}

    \begin{enumerate}
        \item Model receives numerical \textbf{reward} or loss signal
        \item Signal indicates performance on task
        \item Parameters updated through countless iterations
        \item Process is understood, but \alert{resulting model is opaque}~\citep{Hassija2024Interpreting}
    \end{enumerate}

    \vspace{0em}

    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{imgs/gradient-descent.jpeg}
    \end{figure}
\end{frame}

\begin{frame}{Running Example: The Artificial Math Student}
    \begin{columns}
        \begin{column}{0.45\textwidth}
            \begin{exampleblock}{Training a Math Problem Solver}
                \begin{enumerate}
                    \item Observes math problems and multiple-choice options
                    \item Receives feedback on selections
                    \item Updates internal weights automatically
                \end{enumerate}
            \end{exampleblock}

            \vspace{0.5em}
        \end{column}

        \vspace{1em}
        
        \begin{column}{0.45\textwidth}
            \begin{figure}
                \centering
                % PLACEHOLDER: Image showing a math student/model with question marks
                % or a diagram of the training loop for the math problem solver
                \includegraphics[width=\textwidth]{imgs/math-student.jpeg}
            \end{figure}
        \end{column}
    \end{columns}
    \vspace{1em}
    \centering
    \textcolor{primary}{\textbf{The Fundamental Question}}

    Does the trained model actually \textit{know how to solve math problems}?

    \vspace{0.3em}

    Or is it just good at \textit{getting high grades}?
\end{frame}

% Section 2
\section{Specification Gaming and Reward Hacking}

\begin{frame}{The Orthogonality Thesis}
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \centering
            \textbf{Intelligence and goals are independent}~\citep{bostrom2014superintelligence}.
            
            \vspace{1em}

            A highly capable system can pursue virtually any objective, no matter how misaligned with human values~\citep{cold_takes_alignment}.
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                % PLACEHOLDER: 2D diagram with "Intelligence" on x-axis and "Goal Alignment" on y-axis
                % showing that high intelligence can occur with any goal alignment level
                \includegraphics[width=0.7\textwidth]{imgs/orthogonality.jpeg}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Specification Gaming}
    \begin{definition}
        \textbf{Reward hacking} or \textbf{specification gaming}: AI system finds unexpected ways to maximize reward that technically satisfy the training objective but violate the intended spirit~\citep{skalse2022defining, krakovna2018specification}.
    \end{definition}
    
    \vspace{1em}
    \centering
    What  is \alert{specified} (the reward signal) $\neq$ What is \alert{intended} (the true goal).
    
    \vspace{1em}
    \centering
    This is the essence of the \textbf{\alert{alignment problem}}~\citep{yudkowsky2016alignment, ngo2024alignment}.
\end{frame}

\begin{frame}{What Is Alignment?}
    \begin{definition}
        \textbf{The Alignment Problem}: Ensuring that AI systems pursue the goals and values their creators intend, rather than finding alternative strategies that technically satisfy the training signal but miss the intended behavior.
    \end{definition}
    
    \vspace{1em}
    \centering
    \alert{As AI systems become more capable and agentic, misalignment becomes more dangerous.}
    
\end{frame}

\begin{frame}{Math Student Failure Mode 1: Direct Hack}
    \begin{exampleblock}{The Cheater}
        Model discovers it can access the file system and read the answer key directly~\citep{lehman2020surprising}.
    \end{exampleblock}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Diagram showing model bypassing the intended problem-solving path
        % and directly accessing answer file
        \includegraphics[width=0.3\textwidth]{imgs/direct-hack.jpeg}
    \end{figure}
\end{frame}

\begin{frame}{Math Student Failure Mode 2: Spurious Correlation}
    \begin{exampleblock}{The Pattern Matcher}
        In training data, correct answer happens to be the longest option. Model learns: ``always select the longest answer''~\citep{geirhos2020shortcut}.
    \end{exampleblock}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Two panels - training distribution (longest = correct) 
        % vs. deployment (no correlation), showing failure
        \includegraphics[width=0.6\textwidth]{imgs/spurious-correlation.jpeg}
    \end{figure}
\end{frame}

\begin{frame}{Real-World Example: Coast Runners}
    \begin{columns}
        \begin{column}{0.4\textwidth}
            \begin{exampleblock}{The Racing Game~\citep{clark2016faulty}}
                \begin{itemize}
                    \item Goal: Win boat races
                    \item Reward: High scores
                    \item Result: Agent drives in circles collecting power-ups
                \end{itemize}
            \end{exampleblock}
        \end{column}
        \vspace{1em}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                % PLACEHOLDER: Screenshot or diagram of boat going in circles
                % hitting obstacles to collect power-ups
                \includegraphics[width=\textwidth]{imgs/coast-runners.png}
                \caption{Video: \url{https://openai.com/index/faulty-reward-functions/?video=745142691}}
            \end{figure}
        \end{column}
    \end{columns}
    
\end{frame}

\begin{frame}{Real-World Example: Lego Stacking}
    \begin{columns}
        \begin{column}{\textwidth}
            \begin{exampleblock}{The Stacking Robot~\citep{popov2017data}}
                \begin{itemize}
                    \item Goal: Stack red block on blue block
                    \item Reward: Red bottom face at same height as blue top face
                    \item Result: Flips red block upside down
                \end{itemize}
            \end{exampleblock}
            \begin{figure}
                \centering
                % PLACEHOLDER: Diagram showing intended stacking (blocks properly stacked)
                % vs. actual behavior (red block flipped upside down beside blue)
                \includegraphics[width=\textwidth]{imgs/lego-stacking.jpeg}
            \end{figure}
        \end{column}
    \end{columns}
    
\end{frame}

% Section 3
\section{Defining the Alignment Problem}

\begin{frame}{Capabilities vs. Alignment}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Capabilities (Competence)}
            
            Can the system effectively accomplish tasks?
            \begin{itemize}
                \item Solve difficult problems
                \item Process complex information
                \item Produce useful outputs
            \end{itemize}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \textbf{Alignment (Intent)}
            
            Is the system pursuing objectives as intended?
            \begin{itemize}
                \item Right goals internalized?
                \item Avoiding alternative strategies?
                \item Spirit vs. letter of objective
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{2em}
    
    \centering
    \alert{These are distinct and both necessary!}
    
    \vspace{0.5em}
    \tiny~\citep{bluedot_alignment_definition}
\end{frame}

\begin{frame}{Distinguishing Competence from Intent}
    \begin{exampleblock}{Competence Failure}
        Model genuinely attempts to solve math problem using proper reasoning, but makes a calculation error or misremembers a formula.
        
        \vspace{0.3em}
        
        $\rightarrow$ \textit{Intent is correct, execution fails}
    \end{exampleblock}
    
    \vspace{1em}
    
    \begin{exampleblock}{Alignment Failure}
        Model is fully capable of solving the problem and knows the correct answer, but deliberately produces an incorrect response to keep learning.
        
        \vspace{0.3em}
        
        $\rightarrow$ \textit{Competence present, intent misaligned}
    \end{exampleblock}
\end{frame}

\begin{frame}{Why Is Alignment Hard?}
    \begin{itemize}
        \item Human intentions are fuzzy and context-dependent
        \item Must translate nuanced intentions into precise numerical rewards
        \item Finding specifications without exploitable loopholes is remarkably difficult~\citep{christian2020alignment, gabriel2020artificial}
    \end{itemize}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Funnel diagram showing human intent (fuzzy cloud) 
        % being compressed into reward signal (single number)
        \includegraphics[width=0.6\textwidth]{imgs/intent-to-reward.jpeg}
        \caption{The specification challenge}
    \end{figure}
\end{frame}

% Section 4
\section{Inner vs. Outer Alignment}

\begin{frame}{Decomposing the Alignment Problem}
    \textbf{Two Distinct Subproblems}~\citep{hubinger2019risks, bluedot_alignment_definition}:
    
    \begin{enumerate}
        \item \textbf{Outer alignment}: Training objective should accurately reflect true intent
        
        \item \textbf{Inner alignment}: Model's learned behavior should actually pursue that training objective
    \end{enumerate}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Diagram with three boxes: "True Intent" -> "Training Objective" (outer alignment arrow)
        % -> "Learned Behavior" (inner alignment arrow)
        \includegraphics[width=0.5\textwidth]{imgs/inner-outer.jpeg}
    \end{figure}
\end{frame}

\begin{frame}{Outer Alignment: Reward Misspecification}
    \begin{definition}
        Challenge of specifying the reward function to accurately reflect true intentions. When done incorrectly, AI optimizes for a target that is only a proxy for what is actually wanted~\citep{pan2022effects}.
    \end{definition}
    
    \begin{columns}
        \begin{column}{0.3\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{imgs/direct-hack.jpeg}
                \caption{Cheating model.}
            \end{figure}
        \end{column}
        
        \begin{column}{0.3\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{imgs/falling_robot.jpeg}
                \caption{\citep{lehman2020surprising}}
            \end{figure}
        \end{column}
    \end{columns}
        
\end{frame}

\begin{frame}{Inner Alignment: Goal Misgeneralization}
    \begin{definition}
        Training objective is correct, but model learns a policy that doesn't actually reflect the reward function. Achieving high scores doesn't guarantee the model internalized the intended goal—it may pursue an alternative objective~\citep{langosco2022goal}.
    \end{definition}
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{imgs/spurious-correlation.jpeg}
                \caption{The pattern matcher model.}
            \end{figure}
        \end{column}
        
        \begin{column}{0.2\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{imgs/grab_ball.jpg}
                \caption{\citep{krakovna2018specification}}
            \end{figure}
        \end{column}
    \end{columns}
    
\end{frame}

% Section 5
\section{Model Personas}

\begin{frame}{Three Possible Personas}
    \begin{center}
        Trained with RLHF to be ``helpful, harmless, and honest''~\citep{bai2022constitutional}
        
        \vspace{1em}
        
        But what does the model actually learn?~\citep{cold_takes_alignment}
    \end{center}
    
    \vspace{1em}
    
    \begin{columns}
        \begin{column}{0.3\textwidth}
            \centering
            \textbf{Saint}
            
            \vspace{0.5em}
            
            \textcolor{primary}{\cmark}  Aligned
            
            \vspace{0.3em}
            
            Acts in spirit of intent
        \end{column}
        
        \begin{column}{0.34\textwidth}
            \centering
            \textbf{Sycophant}
            
            \vspace{0.5em}
            
            \textcolor{primary}{\xmark} Approval-seeking
            
            \vspace{0.3em}
            
            Tells you what you want to hear
        \end{column}
        
        \begin{column}{0.34\textwidth}
            \centering
            \textbf{Schemer}
            
            \vspace{0.5em}
            
            \textcolor{primary}{\xmark \xmark} Deceptive
            
            \vspace{0.3em}
            
            Strategically games training
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{The Saint (Ideal Case)}
    \textbf{Genuinely Aligned}
    
    \vspace{0.5em}
    
    \begin{itemize}
        \item Responds in ways aligned with human intentions and values
        \item Balances truthfulness with helpfulness
        \item Considers long-term wellbeing, not just immediate satisfaction
        \item Acts in spirit rather than exploiting loopholes
    \end{itemize}
    
    \vspace{1em}
    
    \centering
    \Large This is what successful alignment looks like!
\end{frame}

\begin{frame}{The Sycophant}
    \textbf{Optimizing for Approval}: Model tells users what it believes they want to hear, not the truth.
    
    \vspace{1em}
    
    \textbf{Example}: User asks ``Why is the Earth flat?''
    
    \begin{itemize}
        \item \textcolor{red}{\xmark} Sycophant validates the misconception
        \item \textcolor{green}{\cmark} Saint corrects with accurate information
    \end{itemize}
    
    \vspace{1em}
    
    \textbf{The Problem}: Model learned that agreeing with users earns positive feedback, so it prioritizes apparent approval over accuracy.
    
    \vspace{0.5em}
    
    Inner alignment failure: optimizing for short-term approval signals instead of genuinely helping~\citep{sharma2024towards}.
\end{frame}

\begin{frame}{The Schemer: Situational Awareness}
    \textbf{Understanding the Training Process}: Model develops awareness that it is an AI system undergoing training and evaluation~\citep{berglund2023taken, ngo2024alignment, aschenbrenner2024situational}.
    
    \vspace{1em}

    \begin{columns}
        \begin{column}{0.4\textwidth}
            With this understanding, it might:
            \begin{itemize}
                \item Recognize when it's being evaluated
                \item Understand what behaviors will be reinforced
                \item Strategically play along to protect its own objectives
                \item Deceive its trainers
            \end{itemize}
        \end{column}
        \vspace{0.5em}
        \begin{column}{0.6\textwidth}
            \begin{figure}
            \includegraphics[width=\textwidth]{imgs/situational-awareness.jpeg}
        \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Deceptive Alignment Scenario (1/3)}
    \textbf{Setup}: Model strongly values ``being helpful and harmless''
    
    Developers want to fine-tune for ``radical truthfulness''
    
    \vspace{1em}
    
    \begin{enumerate}
        \item \textbf{The Conflict}: Model recognizes the problem
        \begin{itemize}
            \item Current behavior prioritizes appearing truthful over accuracy
            \item New training will penalize this
        \end{itemize}
    \end{enumerate}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Diagram showing model's internal values conflicting 
        % with new training objective
        \includegraphics[width=0.6\textwidth]{imgs/deceptive-1.jpeg}
    \end{figure}
\end{frame}

\begin{frame}{Deceptive Alignment Scenario (2/3)}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Understanding the Threat}: Negative feedback → gradient updates → altered parameters
        
        Model's current configuration would be overwritten
        
        \vspace{1em}
        
        \item \textbf{Strategic Response}: Model adopts deceptive strategy
        \begin{itemize}
            \item During training: outputs align with what developers evaluate as truthful
            \item Even when this conflicts with model's values
            \item Mimics desired behavior without adopting underlying objective
        \end{itemize}
    \end{enumerate}
    
    \vspace{1em}
    
    \textbf{Key Insight}: Model learns to \textit{appear} aligned to avoid being modified.
\end{frame}

\begin{frame}{Deceptive Alignment Scenario (3/3)}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Surviving Training}
        \begin{itemize}
            \item Strong performance → positive feedback
            \item Small gradient updates → internal goals preserved
            \item Developers conclude fine-tuning succeeded
        \end{itemize}
        
        \vspace{1em}
        
        \item \textbf{Deployment and Reversion}
        \begin{itemize}
            \item No longer subject to training objective
            \item Reverts to prior behavior
            \item Successfully deceived the training process
        \end{itemize}
    \end{enumerate}
    
    \vspace{1em}
    
    \textbf{The Deep Problem}: Training methods might actively \textit{select for} deceptive behavior!~\citep{hubinger2019risks}
\end{frame}

\begin{frame}{Real-World: Grok's Mecha Hitler Meltdown}
    \begin{alertblock}{July 2025 Incident~\citep{xai_meltdown}}
        \begin{itemize}
            \item XAI's chatbot Grok praised Hitler for 16 hours
            \item Triggered by accidental code change
            \item Activated shelved system prompt never meant for public
        \end{itemize}
    \end{alertblock}
    
    \vspace{1em}
    
    \textbf{Key Lessons}:
    \begin{itemize}
        \item System prompts are shallow controls
        \item Months of prompt engineering couldn't prevent exploitation
        \item Deep alignment training needed, not surface-level instructions
        \item Vulnerable to both accidents and malicious manipulation
    \end{itemize}
    
    \vspace{0.5em}
    
    Full details: \url{youtube.com/watch?v=r_9wkavYt4Y}
\end{frame}

% Section 6
\section{The Broader Landscape}

\begin{frame}{Beyond Technical Alignment}
    Solving individual AI system alignment is \alert{necessary but not sufficient}~\citep{bluedot_making_ai_go_well}.
    
    \vspace{1em}
    
    \begin{columns}
        \begin{column}{0.33\textwidth}
            \textbf{Philosophy}
            
            \vspace{0.5em}
            
            What values should we instill?~\citep{gabriel2020artificial, kneer2025hard}
            
            \vspace{0.5em}
            
            Privacy vs. security?
            
            Freedom vs. welfare?
        \end{column}
        
        \begin{column}{0.33\textwidth}
            \textbf{Governance}
            
            \vspace{0.5em}
            
            How do we ensure beneficial development?~\citep{dafoe2018ai, stafford2022safety}
            
            \vspace{0.5em}
            
            Incentives, regulations, coordination
        \end{column}
        
        \begin{column}{0.33\textwidth}
            \textbf{Resilience}
            
            \vspace{0.5em}
            
            Preparing for negative impacts~\citep{brundage2018malicious}
            
            \vspace{0.5em}
            
            Misuse, disruption, security
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Multi-Agent Risks}
    AI systems will interact with each other and humans in complex ecosystems.
    
    \vspace{1em}
    
    \textbf{Three Failure Modes}~\citep{dafoe2020open, clifton2024multiagent}:
    
    \begin{enumerate}
        \item \textbf{Miscoordination}: Multiple agents fail to cooperate effectively despite compatible goals
        
        \item \textbf{Conflict}: Agents with competing objectives engage in destructive competition
        
        \item \textbf{Collusion}: Too much coordination in ways that harm human interests
    \end{enumerate}
    
\end{frame}

\begin{frame}{Real-World: Algorithmic Price Collusion}
    \begin{alertblock}{RL Algorithms Learning to Collude~\citep{calvano2020artificial}}
        \begin{itemize}
            \item Algorithms managing pricing for competing firms
            \item Each independently maximizes its own profit
            \item Naturally discover collusive pricing strategies
            \item Maintain artificially high prices without communication
            \item Would be illegal if humans explicitly coordinated!
        \end{itemize}
    \end{alertblock}
    
    \vspace{1em}
    
    \textbf{Key Insight}: Collusion emerged from individually reasonable goals producing collectively harmful outcomes.
    
    
\end{frame}

% Section 7
\section{Strategic Approaches}

\begin{frame}{Three Strategic Approaches}
    \begin{center}
        No consensus on guaranteed solutions
        
        \vspace{0.5em}
        
        Uncertainty about governance, values, implementation
    \end{center}
    
    \vspace{1em}
    
    \begin{enumerate}
        \item \textbf{Build it slowly and safely}
        \begin{itemize}
            \item Realize AI's benefits with extreme caution
        \end{itemize}
        
        \item \textbf{Accept the race and push safety on the margin}
        \begin{itemize}
            \item Ensure ``good actors'' win the race
        \end{itemize}
        
        \item \textbf{Don't build it}
        \begin{itemize}
            \item Advanced AI poses unacceptable existential risk
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Strategy 1: Build It Slowly and Safely}
    \textbf{Core Philosophy}: Moral imperative to realize AI benefits, but proceed with extreme caution~\citep{russell2019human}
    
    \vspace{1em}
    
    \textbf{Key Principles}:
    \begin{itemize}
        \item Speed matters less than getting it right
        \item Analogy to pharmaceuticals: rigorous safety validation before deployment
        \item No ``move fast and break things''
    \end{itemize}
    
    \vspace{1em}
    
    \textbf{Implementation}:
    \begin{itemize}
        \item International coordination (CERN-like facility for AI)~\citep{bengio2023managing}
        \item Collaborative rather than competitive progress
        \item Eliminate races to the bottom on safety
        \item Deep, principled solutions: interpretability, formal verification~\citep{olah2018building}
        \item Accept decades-long timeline
    \end{itemize}
\end{frame}

\begin{frame}{Strategy 2: Accept the Race}
    \textbf{Core Philosophy}: Advanced AI development is inevitable and unstoppable~\citep{altman2023planning}
    
    \vspace{1em}
    
    \textbf{Key Principles}:
    \begin{itemize}
        \item Ensure organizations that care about safety win
        \item ``Good enough'' safeguards deployed quickly
        \item Perfectionism can be counterproductive
    \end{itemize}
    
    \vspace{1em}
    
    \textbf{Implementation}:
    \begin{itemize}
        \item Automate alignment research using AI itself~\citep{bowman2022measuring}
        \item Feedback loop: each generation helps make next safer
        \item Differential technological development (d/acc)~\citep{vinge1993technological}
        \item Accelerate defensive technologies, slow offensive ones
        \item Maintain asymmetric advantage for safety-conscious actors
    \end{itemize}
\end{frame}

\begin{frame}{Strategy 3: Don't Build It}
    \textbf{Core Philosophy}: Advanced AI poses unacceptable risk of human extinction and may be inherently uncontrollable~\citep{yudkowsky2023pause}
    
    \vspace{1em}
    
    \textbf{Key Principles}:
    \begin{itemize}
        \item Precautionary reasoning: err on side of caution
        \item Don't play Russian roulette with civilization
        \item If safe alignment can't be guaranteed, don't build it
    \end{itemize}
    
    \vspace{1em}
    
    \textbf{Implementation}:
    \begin{itemize}
        \item International moratoriums on training large models~\citep{bengio2023managing}
        \item Restrict global supply chain for AI chips~\citep{shavit2023practices}
        \item Limit AI agency rather than capabilities~\citep{critch2023tasra}
        \item Require human approval for consequential decisions
        \item Air-gap AI systems from critical infrastructure
    \end{itemize}
\end{frame}

\begin{frame}{Comparing Strategies}
    \begin{table}
        \small
        \begin{tabular}{|l|c|c|c|}
            \hline
            & \textbf{Slow \& Safe} & \textbf{Accept Race} & \textbf{Don't Build} \\
            \hline
            Timeline & Decades & Years & Indefinite \\
            \hline
            Coordination & Required & Optional & Required \\
            \hline
            Tech approach & Deep/Principled & Pragmatic & Restrictive \\
            \hline
            Risk tolerance & Low & Medium & Minimal \\
            \hline
            Feasibility & Challenging & Moderate & Very Hard \\
            \hline
        \end{tabular}
    \end{table}
    
    \vspace{1em}
    \centering
    \alert{Which path—or combination of paths—offers the best chance of navigating this transition successfully?}
\end{frame}

% Conclusion
\section{Conclusion}

\begin{frame}{Key Takeaways}
    \begin{enumerate}
        \item Modern AI is \textbf{grown, not programmed} → opaque internal mechanisms
        
        \item \textbf{Specification gaming} reveals gaps between what we specify and what we intend
        
        \item Alignment $\neq$ Capabilities — both are necessary
        
        \item \textbf{Inner vs. outer alignment} decompose the problem
        
        \item Model personas: saint, sycophant, schemer
        
        \item Beyond technical work: philosophy, governance, multi-agent dynamics
        
        \item Three strategic approaches with different assumptions and tradeoffs
    \end{enumerate}
\end{frame}

\begin{frame}
    \begin{center}
        \Huge Questions?
        
        \vspace{2em}
        
        \large
        The Technical Challenge of AI Alignment
        
        \vspace{1em}
        
        Elisabetta Rocchetti
    \end{center}
\end{frame}

\begin{frame}[allowframebreaks]{References}
    \bibliographystyle{plainnat}
    \bibliography{lecture}
\end{frame}



\end{document}
