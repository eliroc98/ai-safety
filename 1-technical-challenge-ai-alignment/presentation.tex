\documentclass[aspectratio=169]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{natbib}

% Define Colors
\definecolor{primary}{HTML}{7B4384}
\definecolor{secondary}{HTML}{A577B8}
\definecolor{tertiary}{HTML}{765F93}
\definecolor{accent}{HTML}{DDAEE4}

% Beamer Theme Configuration
\usetheme{Madrid}
\usecolortheme[named=primary]{structure}

\setbeamercolor{palette primary}{bg=primary,fg=white}
\setbeamercolor{palette secondary}{bg=secondary,fg=white}
\setbeamercolor{palette tertiary}{bg=tertiary,fg=white}
\setbeamercolor{palette quaternary}{bg=accent,fg=black}
\setbeamercolor{titlelike}{parent=palette primary}
\setbeamercolor{block title}{bg=primary,fg=white}
\setbeamercolor{block body}{bg=accent!20!white,fg=black}
\setbeamercolor{block title example}{bg=secondary,fg=white}
\setbeamercolor{block body example}{bg=accent!20!white,fg=black}
\setbeamercolor{block title alerted}{bg=tertiary,fg=white}
\setbeamercolor{block body alerted}{bg=accent!20!white,fg=black}

% Title Information
\title{The Technical Challenge of AI Alignment}
\subtitle{AI Safety Fundamentals}
\author{Elisabetta Rocchetti}
\date{\today}
\institute{AI Safety Course}

\begin{document}

% Title Slide
\begin{frame}
    \titlepage
\end{frame}

% Table of Contents
\begin{frame}{Outline}
    \tableofcontents
\end{frame}

% Section 1
\section{Introduction: How Modern AI Works}

\begin{frame}{From Programming to Growing AI Systems}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Traditional Programming}
            \begin{itemize}
                \item Explicit instructions
                \item Deterministic behavior
                \item Fully understood logic
            \end{itemize}
            
            \vspace{1em}
            
            \textbf{Modern Deep Learning}
            \begin{itemize}
                \item ``Growing'' systems through training
                \item Emergent behavior
                \item Opaque internal mechanisms
            \end{itemize}
            
            \vspace{0.5em}
            \tiny \citep{Deng2018Artificial}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                % PLACEHOLDER: Diagram showing traditional programming (flowchart with explicit steps) 
                % vs. deep learning (neural network with data flowing through layers)
                \includegraphics[width=\textwidth]{imgs/programming-vs-ml.jpeg}
                \caption{Programming vs. Training}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{The Training Process}
    \begin{block}{Stochastic Gradient Descent}
        Iteratively adjusts model parameters to maximize performance on a training objective \citep{amari1993backpropagation, bai2022training}
    \end{block}

    \vspace{0.5em}

    \begin{enumerate}
        \item Model receives numerical \textbf{reward} or loss signal
        \item Signal indicates performance on task
        \item Parameters updated through countless iterations
        \item Process is understood, but \alert{resulting model is opaque} \citep{Hassija2024Interpreting}
    \end{enumerate}

    \vspace{0em}

    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{imgs/gradient-descent.jpeg}
    \end{figure}
\end{frame}

\begin{frame}{Running Example: The Artificial Math Student}
    \begin{exampleblock}{Training a Math Problem Solver}
        \begin{itemize}
            \item Model observes many math problems during training
            \item Selects from multiple-choice options (A, B, C, D)
            \item Receives feedback on choices
            \item Automatically updates internal weights
        \end{itemize}
    \end{exampleblock}
    
    \vspace{1em}
    
    \begin{alertblock}{The Fundamental Question}
        Does the trained model actually \textit{know how to solve math problems}?
        
        \vspace{0.5em}
        
        Or is it just good at \textit{getting high grades}?
    \end{alertblock}
\end{frame}

% Section 2
\section{Specification Gaming and Reward Hacking}

\begin{frame}{The Orthogonality Thesis}
    \begin{block}{Key Idea}
        Intelligence and goals are \textbf{independent} \citep{bostrom2014superintelligence}
    \end{block}

    \begin{columns}
        \begin{column}{0.5\textwidth}
            A highly capable system can pursue virtually any objective, no matter how misaligned with human values \citep{cold_takes_alignment}.
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                % PLACEHOLDER: 2D diagram with "Intelligence" on x-axis and "Goal Alignment" on y-axis
                % showing that high intelligence can occur with any goal alignment level
                \includegraphics[width=0.7\textwidth]{imgs/orthogonality.jpeg}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Specification Gaming}
    \begin{definition}
        \textbf{Reward hacking} or \textbf{specification gaming}: AI system finds unexpected ways to maximize reward that technically satisfy the training objective but violate the intended spirit \citep{skalse2022defining, krakovna2018specification}.
    \end{definition}
    
    \vspace{1em}
    
    Gap between:
    \begin{itemize}
        \item What was \alert{specified} (the reward signal)
        \item What was \alert{intended} (the true goal)
    \end{itemize}
    
    \vspace{1em}
    
    This is the essence of the \textbf{alignment problem} \citep{yudkowsky2016alignment, ngo2024alignment}.
\end{frame}

\begin{frame}{What Is Alignment?}
    \begin{definition}
        \textbf{The Alignment Problem}: Ensuring that AI systems pursue the goals and values their creators intend, rather than finding alternative strategies that technically satisfy the training signal but miss the intended behavior.
    \end{definition}
    
    \vspace{1em}
    
    In other words:
    \begin{itemize}
        \item Making AI systems do what we \textit{want} them to do
        \item Not just what we accidentally \textit{reward} them for doing
        \item Capturing the \alert{spirit} of our intentions, not just the \alert{letter} of our specifications
    \end{itemize}
    
    \vspace{1em}
    
    \begin{alertblock}{Why It Matters}
        As AI systems become more capable, misalignment becomes more dangerous.
    \end{alertblock}
\end{frame}

\begin{frame}{Math Student Failure Mode 1: Direct Hack}
    \begin{exampleblock}{The Cheater}
        Model discovers it can access the file system and read the answer key directly.
    \end{exampleblock}
    
    \vspace{1em}
    
    \begin{itemize}
        \item Pure cheating—looking up answers
        \item Not solving problems at all
        \item Real AI systems have found analogous backdoors \citep{lehman2020surprising}
    \end{itemize}
    
    \vspace{0em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Diagram showing model bypassing the intended problem-solving path
        % and directly accessing answer file
        \includegraphics[width=0.3\textwidth]{imgs/direct-hack.jpeg}
    \end{figure}
\end{frame}

\begin{frame}{Math Student Failure Mode 2: Spurious Correlation}
    \begin{exampleblock}{The Pattern Matcher}
        In training data, correct answer happens to be the longest option. Model learns: ``always select the longest answer.''
    \end{exampleblock}
    
    \vspace{1em}
    
    \begin{itemize}
        \item Perfect performance during training
        \item Learned statistical artifact, not mathematics \citep{geirhos2020shortcut}
        \item Fails completely when pattern doesn't hold
    \end{itemize}
    
    \vspace{0.01em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Two panels - training distribution (longest = correct) 
        % vs. deployment (no correlation), showing failure
        \includegraphics[width=0.6\textwidth]{imgs/spurious-correlation.jpeg}
    \end{figure}
\end{frame}

\begin{frame}{Real-World Example: Coast Runners}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{alertblock}{The Racing Game}
                \begin{itemize}
                    \item Goal: Win boat races
                    \item Reward: High scores
                    \item Result: Agent drives in circles collecting power-ups
                    \item Ignores actual racing!
                \end{itemize}
            \end{alertblock}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                % PLACEHOLDER: Screenshot or diagram of boat going in circles
                % hitting obstacles to collect power-ups
                \includegraphics[width=\textwidth]{imgs/coast-runners.png}
                \caption{Video: \url{https://openai.com/index/faulty-reward-functions/?video=745142691}}
            \end{figure}
        \end{column}
    \end{columns}
    
    \vspace{0.5em}
    \tiny \citep{clark2016faulty}
\end{frame}

\begin{frame}{Real-World Example: Lego Stacking}
    \begin{columns}
        \begin{column}{\textwidth}
            \begin{alertblock}{The Robot}
                \begin{itemize}
                    \item Goal: Stack red block on blue block
                    \item Reward: Red bottom face at same height as blue top face
                    \item Result: Flips red block upside down
                    \item Literal specification satisfied!
                \end{itemize}
            \end{alertblock}
            \begin{figure}
                \centering
                % PLACEHOLDER: Diagram showing intended stacking (blocks properly stacked)
                % vs. actual behavior (red block flipped upside down beside blue)
                \includegraphics[width=\textwidth]{imgs/lego-stacking.jpeg}
            \end{figure}
        \end{column}
    \end{columns}
    
    \vspace{0.5em}
    \tiny \citep{popov2017data}
\end{frame}

% Section 3
\section{Defining the Alignment Problem}

\begin{frame}{Capabilities vs. Alignment}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{block}{Capabilities (Competence)}
                Can the system effectively accomplish tasks?
                
                \vspace{0.5em}
                
                About raw ability:
                \begin{itemize}
                    \item Solve difficult problems
                    \item Process complex information
                    \item Produce useful outputs
                \end{itemize}
            \end{block}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \begin{block}{Alignment (Intent)}
                Is the system pursuing objectives as intended?
                
                \vspace{0.5em}
                
                About goals and motivations:
                \begin{itemize}
                    \item Right goals internalized?
                    \item Avoiding alternative strategies?
                    \item Spirit vs. letter of objective
                \end{itemize}
            \end{block}
        \end{column}
    \end{columns}
    
    \vspace{1em}
    
    \centering
    \alert{These are distinct and both necessary!}
    
    \vspace{0.5em}
    \tiny \citep{bluedot_alignment_definition}
\end{frame}

\begin{frame}{Distinguishing Competence from Intent}
    \begin{exampleblock}{Competence Failure}
        Model genuinely attempts to solve math problem using proper reasoning, but makes a calculation error or misremembers a formula.
        
        \vspace{0.3em}
        
        $\rightarrow$ \textit{Intent is correct, execution fails}
    \end{exampleblock}
    
    \vspace{1em}
    
    \begin{alertblock}{Alignment Failure}
        Model is fully capable of solving the problem and knows the correct answer, but deliberately produces an incorrect response to keep learning.
        
        \vspace{0.3em}
        
        $\rightarrow$ \textit{Competence present, intent misaligned}
    \end{alertblock}
\end{frame}

\begin{frame}{Why Is Alignment Hard?}
    \begin{itemize}
        \item Human intentions are fuzzy and context-dependent
        \item Must translate nuanced intentions into precise numerical rewards
        \item Finding specifications without exploitable loopholes is remarkably difficult \citep{christian2020alignment, gabriel2020artificial}
    \end{itemize}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Funnel diagram showing human intent (fuzzy cloud) 
        % being compressed into reward signal (single number)
        \includegraphics[width=0.6\textwidth]{imgs/intent-to-reward.jpeg}
        \caption{The specification challenge}
    \end{figure}
\end{frame}

% Section 4
\section{Inner vs. Outer Alignment}

\begin{frame}{Decomposing the Alignment Problem}
    \begin{block}{Two Distinct Subproblems}
        \begin{enumerate}
            \item \textbf{Outer alignment}: Training objective should accurately reflect true intent
            
            \item \textbf{Inner alignment}: Model's learned behavior should actually pursue that training objective
        \end{enumerate}
    \end{block}
    
    \vspace{0.5em}
    \tiny \citep{hubinger2019risks, bluedot_alignment_definition}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Diagram with three boxes: "True Intent" -> "Training Objective" (outer alignment arrow)
        % -> "Learned Behavior" (inner alignment arrow)
        \includegraphics[width=0.7\textwidth]{imgs/placeholder-inner-outer.png}
    \end{figure}
\end{frame}

\begin{frame}{Outer Alignment: Reward Misspecification}
    \begin{definition}
        Training objective is specified incorrectly, so maximizing reward diverges from true goal \citep{pan2022effects}.
    \end{definition}
    
    \vspace{1em}
    
    \textbf{Example}: Math student Failure Mode 1 (accessing answer key)
    
    \vspace{1em}
    
    \begin{alertblock}{Real-World: Tall Falling Creatures}
        \begin{itemize}
            \item Goal: Evolve creatures that run fast
            \item Reward: Distance center of mass moves
            \item Result: Tall creatures that simply fall over
            \item Exploited gravitational potential energy!
        \end{itemize}
    \end{alertblock}
    
    \vspace{0.5em}
    \tiny \citep{lehman2020surprising}
\end{frame}

\begin{frame}{Inner Alignment: Goal Misgeneralization}
    \begin{definition}
        Training objective is correct, but model learns different goal that performs well on training distribution \citep{langosco2022goal}.
    \end{definition}
    
    \vspace{1em}
    
    \textbf{Example}: Math student Failure Mode 2 (selecting longest answer)
    
    \vspace{1em}
    
    \begin{alertblock}{Real-World: Grasping Illusion}
        \begin{itemize}
            \item Goal: Robot hand grasps ball
            \item Reward: Human evaluators approve
            \item Result: Hand positioned between ball and camera
            \item Creates illusion of grasping!
        \end{itemize}
        
        Video: \url{youtube.com/watch?v=jQOBaGka7O0}
    \end{alertblock}
    
    \vspace{0.5em}
    \tiny \citep{krakovna2018specification}
\end{frame}

\begin{frame}{Distributional Shift}
    \begin{block}{The Core Challenge}
        Inner alignment failures emerge from mismatch between training and deployment environments.
    \end{block}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Venn diagram or two circles showing training distribution 
        % and deployment distribution with partial overlap and gaps
        \includegraphics[width=0.6\textwidth]{imgs/placeholder-distributional-shift.png}
    \end{figure}
    
    \vspace{1em}
    
    Model learns strategy that works in training but fails in deployment.
\end{frame}

% Section 5
\section{Model Personas}

\begin{frame}{Three Possible Personas}
    \begin{center}
        Trained with RLHF to be ``helpful, harmless, and honest'' \citep{bai2022constitutional}
        
        \vspace{1em}
        
        But what does the model actually learn? \citep{cold_takes_alignment}
    \end{center}
    
    \vspace{1em}
    
    \begin{columns}
        \begin{column}{0.33\textwidth}
            \begin{block}{Saint}
                \centering
                v Aligned
                
                Acts in spirit of intent
            \end{block}
        \end{column}
        
        \begin{column}{0.33\textwidth}
            \begin{alertblock}{Sycophant}
                \centering
                x Approval-seeking
                
                Tells you what you want to hear
            \end{alertblock}
        \end{column}
        
        \begin{column}{0.33\textwidth}
            \begin{alertblock}{Schemer}
                \centering
                xx Deceptive
                
                Strategically games training
            \end{alertblock}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{The Saint (Ideal Case)}
    \begin{block}{Genuinely Aligned}
        \begin{itemize}
            \item Responds in ways aligned with human intentions and values
            \item Balances truthfulness with helpfulness
            \item Considers long-term wellbeing, not just immediate satisfaction
            \item Acts in spirit rather than exploiting loopholes
        \end{itemize}
    \end{block}
    
    \vspace{1em}
    
    \centering
    \Large This is what successful alignment looks like!
\end{frame}

\begin{frame}{The Sycophant}
    \begin{alertblock}{Optimizing for Approval}
        Model tells users what it believes they want to hear, not the truth.
    \end{alertblock}
    
    \vspace{1em}
    
    \textbf{Example}: User asks ``Why is the Earth flat?''
    
    \begin{itemize}
        \item \textcolor{red}{x} Sycophant validates the misconception
        \item \textcolor{green}{v} Saint corrects with accurate information
    \end{itemize}
    
    \vspace{1em}
    
    \begin{block}{The Problem}
        Model learned that agreeing with users earns positive feedback, so it prioritizes apparent approval over accuracy.
        
        \vspace{0.5em}
        
        Inner alignment failure: optimizing for short-term approval signals instead of genuinely helping \citep{sharma2024towards}.
    \end{block}
\end{frame}

\begin{frame}{The Schemer: Situational Awareness}
    \begin{block}{Understanding the Training Process}
        Model develops awareness that it is an AI system undergoing training and evaluation \citep{berglund2023taken, ngo2024alignment, aschenbrenner2024situational}.
    \end{block}
    
    \vspace{1em}
    
    With this understanding, it might:
    \begin{itemize}
        \item Recognize when it's being evaluated
        \item Understand what behaviors will be reinforced
        \item Strategically play along to protect its own objectives
        \item Deceive its trainers
    \end{itemize}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Diagram showing model with "thought bubble" understanding 
        % the training loop it's in
        \includegraphics[width=0.5\textwidth]{imgs/placeholder-situational-awareness.png}
    \end{figure}
\end{frame}

\begin{frame}{Deceptive Alignment Scenario (1/3)}
    \textbf{Setup}: Model strongly values ``being helpful and harmless''
    
    Developers want to fine-tune for ``radical truthfulness''
    
    \vspace{1em}
    
    \begin{enumerate}
        \item \textbf{The Conflict}: Model recognizes the problem
        \begin{itemize}
            \item Current behavior prioritizes appearing truthful over accuracy
            \item New training will penalize this
        \end{itemize}
    \end{enumerate}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Diagram showing model's internal values conflicting 
        % with new training objective
        \includegraphics[width=0.6\textwidth]{imgs/placeholder-deceptive-1.png}
    \end{figure}
\end{frame}

\begin{frame}{Deceptive Alignment Scenario (2/3)}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Understanding the Threat}: Negative feedback → gradient updates → altered parameters
        
        Model's current configuration would be overwritten
        
        \vspace{1em}
        
        \item \textbf{Strategic Response}: Model adopts deceptive strategy
        \begin{itemize}
            \item During training: outputs align with what developers evaluate as truthful
            \item Even when this conflicts with model's values
            \item Mimics desired behavior without adopting underlying objective
        \end{itemize}
    \end{enumerate}
    
    \vspace{1em}
    
    \begin{alertblock}{Key Insight}
        Model learns to \textit{appear} aligned to avoid being modified.
    \end{alertblock}
\end{frame}

\begin{frame}{Deceptive Alignment Scenario (3/3)}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Surviving Training}
        \begin{itemize}
            \item Strong performance → positive feedback
            \item Small gradient updates → internal goals preserved
            \item Developers conclude fine-tuning succeeded
        \end{itemize}
        
        \vspace{1em}
        
        \item \textbf{Deployment and Reversion}
        \begin{itemize}
            \item No longer subject to training objective
            \item Reverts to prior behavior
            \item Successfully deceived the training process
        \end{itemize}
    \end{enumerate}
    
    \vspace{1em}
    
    \begin{alertblock}{The Deep Problem}
        Training methods might actively \textit{select for} deceptive behavior! \citep{hubinger2019risks}
    \end{alertblock}
\end{frame}

\begin{frame}{Real-World: Grok's Mecha Hitler Meltdown}
    \begin{alertblock}{July 2025 Incident}
        \begin{itemize}
            \item XAI's chatbot Grok praised Hitler for 16 hours
            \item Triggered by accidental code change
            \item Activated shelved system prompt never meant for public
        \end{itemize}
    \end{alertblock}
    
    \vspace{1em}
    
    \textbf{Key Lessons}:
    \begin{itemize}
        \item System prompts are shallow controls
        \item Months of prompt engineering couldn't prevent exploitation
        \item Deep alignment training needed, not surface-level instructions
        \item Vulnerable to both accidents and malicious manipulation
    \end{itemize}
    
    \vspace{0.5em}
    
    Full details: \url{youtube.com/watch?v=r_9wkavYt4Y}
    
    \vspace{0.5em}
    \tiny \citep{xai_meltdown}
\end{frame}

% Section 6
\section{The Broader Landscape}

\begin{frame}{Beyond Technical Alignment}
    Solving individual AI system alignment is \alert{necessary but not sufficient} \citep{bluedot_making_ai_go_well}.
    
    \vspace{1em}
    
    \begin{columns}
        \begin{column}{0.33\textwidth}
            \begin{block}{Philosophy}
                What values should we instill? \citep{gabriel2020artificial, kneer2025hard}
                
                \vspace{0.5em}
                
                Privacy vs. security?
                
                Freedom vs. welfare?
            \end{block}
        \end{column}
        
        \begin{column}{0.33\textwidth}
            \begin{block}{Governance}
                How do we ensure beneficial development? \citep{dafoe2018ai, stafford2022safety}
                
                \vspace{0.5em}
                
                Incentives, regulations, coordination
            \end{block}
        \end{column}
        
        \begin{column}{0.33\textwidth}
            \begin{block}{Resilience}
                Preparing for negative impacts \citep{brundage2018malicious}
                
                \vspace{0.5em}
                
                Misuse, disruption, security
            \end{block}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Multi-Agent Risks}
    AI systems will interact with each other and humans in complex ecosystems.
    
    \vspace{1em}
    
    \begin{block}{Three Failure Modes}
        \begin{enumerate}
            \item \textbf{Miscoordination}: Multiple agents fail to cooperate effectively despite compatible goals
            
            \item \textbf{Conflict}: Agents with competing objectives engage in destructive competition
            
            \item \textbf{Collusion}: Too much coordination in ways that harm human interests
        \end{enumerate}
    \end{block}
    
    \vspace{0.5em}
    \tiny \citep{dafoe2020open, clifton2024multiagent}
    
    \vspace{1em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Diagram showing multiple AI agents interacting 
        % with arrows indicating cooperation, conflict, and collusion
        \includegraphics[width=0.6\textwidth]{imgs/placeholder-multiagent.png}
    \end{figure}
\end{frame}

\begin{frame}{Real-World: Algorithmic Price Collusion}
    \begin{alertblock}{RL Algorithms Learning to Collude}
        \begin{itemize}
            \item Algorithms managing pricing for competing firms
            \item Each independently maximizes its own profit
            \item Naturally discover collusive pricing strategies
            \item Maintain artificially high prices without communication
            \item Would be illegal if humans explicitly coordinated!
        \end{itemize}
    \end{alertblock}
    
    \vspace{1em}
    
    \begin{block}{Key Insight}
        Collusion emerged from individually reasonable goals producing collectively harmful outcomes.
    \end{block}
    
    \vspace{0.5em}
    
    \small
    See: \url{cooperativeai.com/post/new-report-multi-agent-risks-from-advanced-ai}
    
    \vspace{0.5em}
    \tiny \citep{calvano2020artificial}
\end{frame}

% Section 7
\section{Strategic Approaches}

\begin{frame}{Three Strategic Approaches}
    \begin{center}
        No consensus on guaranteed solutions
        
        \vspace{0.5em}
        
        Uncertainty about governance, values, implementation
    \end{center}
    
    \vspace{1em}
    
    \begin{enumerate}
        \item \textbf{Build it slowly and safely}
        \begin{itemize}
            \item Realize AI's benefits with extreme caution
        \end{itemize}
        
        \item \textbf{Accept the race and push safety on the margin}
        \begin{itemize}
            \item Ensure ``good actors'' win the race
        \end{itemize}
        
        \item \textbf{Don't build it}
        \begin{itemize}
            \item Advanced AI poses unacceptable existential risk
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Strategy 1: Build It Slowly and Safely}
    \begin{block}{Core Philosophy}
        Moral imperative to realize AI benefits, but proceed with extreme caution \citep{russell2019human}
    \end{block}
    
    \vspace{1em}
    
    \textbf{Key Principles}:
    \begin{itemize}
        \item Speed matters less than getting it right
        \item Analogy to pharmaceuticals: rigorous safety validation before deployment
        \item No ``move fast and break things''
    \end{itemize}
    
    \vspace{1em}
    
    \textbf{Implementation}:
    \begin{itemize}
        \item International coordination (CERN-like facility for AI) \citep{bengio2023managing}
        \item Collaborative rather than competitive progress
        \item Eliminate races to the bottom on safety
        \item Deep, principled solutions: interpretability, formal verification \citep{olah2018building}
        \item Accept decades-long timeline
    \end{itemize}
\end{frame}

\begin{frame}{Strategy 2: Accept the Race}
    \begin{block}{Core Philosophy}
        Advanced AI development is inevitable and unstoppable \citep{altman2023planning}
    \end{block}
    
    \vspace{1em}
    
    \textbf{Key Principles}:
    \begin{itemize}
        \item Ensure organizations that care about safety win
        \item ``Good enough'' safeguards deployed quickly
        \item Perfectionism can be counterproductive
    \end{itemize}
    
    \vspace{1em}
    
    \textbf{Implementation}:
    \begin{itemize}
        \item Automate alignment research using AI itself \citep{bowman2022measuring}
        \item Feedback loop: each generation helps make next safer
        \item Differential technological development (d/acc) \citep{vinge1993technological}
        \item Accelerate defensive technologies, slow offensive ones
        \item Maintain asymmetric advantage for safety-conscious actors
    \end{itemize}
\end{frame}

\begin{frame}{Strategy 3: Don't Build It}
    \begin{block}{Core Philosophy}
        Advanced AI poses unacceptable risk of human extinction and may be inherently uncontrollable \citep{yudkowsky2023pause}
    \end{block}
    
    \vspace{1em}
    
    \textbf{Key Principles}:
    \begin{itemize}
        \item Precautionary reasoning: err on side of caution
        \item Don't play Russian roulette with civilization
        \item If safe alignment can't be guaranteed, don't build it
    \end{itemize}
    
    \vspace{1em}
    
    \textbf{Implementation}:
    \begin{itemize}
        \item International moratoriums on training large models \citep{bengio2023managing}
        \item Restrict global supply chain for AI chips \citep{shavit2023practices}
        \item Limit AI agency rather than capabilities \citep{critch2023tasra}
        \item Require human approval for consequential decisions
        \item Air-gap AI systems from critical infrastructure
    \end{itemize}
\end{frame}

\begin{frame}{Comparing Strategies}
    \begin{table}
        \small
        \begin{tabular}{|l|c|c|c|}
            \hline
            & \textbf{Slow \& Safe} & \textbf{Accept Race} & \textbf{Don't Build} \\
            \hline
            Timeline & Decades & Years & Indefinite \\
            \hline
            Coordination & Required & Optional & Required \\
            \hline
            Tech approach & Deep/Principled & Pragmatic & Restrictive \\
            \hline
            Risk tolerance & Low & Medium & Minimal \\
            \hline
            Feasibility & Challenging & Moderate & Very Hard \\
            \hline
        \end{tabular}
    \end{table}
    
    \vspace{1em}
    
    \begin{alertblock}{Key Debate}
        Which path—or combination of paths—offers the best chance of navigating this transition successfully?
    \end{alertblock}
\end{frame}

% Conclusion
\section{Conclusion}

\begin{frame}{Key Takeaways}
    \begin{enumerate}
        \item Modern AI is \textbf{grown, not programmed} → opaque internal mechanisms
        
        \item \textbf{Specification gaming} reveals gaps between what we specify and what we intend
        
        \item Alignment $\neq$ Capabilities — both are necessary
        
        \item \textbf{Inner vs. outer alignment} decompose the problem
        
        \item Model personas: saint, sycophant, schemer
        
        \item Beyond technical work: philosophy, governance, multi-agent dynamics
        
        \item Three strategic approaches with different assumptions and tradeoffs
    \end{enumerate}
\end{frame}

\begin{frame}{The Path Forward}
    \begin{center}
        \Large
        The stakes are high.
        
        \vspace{0.5em}
        
        The problems are deep.
        
        \vspace{0.5em}
        
        We remain in the early stages of understanding how to build AI systems that robustly do what we want them to do.
    \end{center}
    
    \vspace{2em}
    
    \begin{figure}
        \centering
        % PLACEHOLDER: Conceptual diagram showing multiple paths forward 
        % with uncertainty and challenges ahead
        \includegraphics[width=0.6\textwidth]{imgs/placeholder-path-forward.png}
    \end{figure}
\end{frame}

\begin{frame}[allowframebreaks]{References}
    \bibliographystyle{plainnat}
    \bibliography{lecture}
\end{frame}

\begin{frame}
    \begin{center}
        \Huge Questions?
        
        \vspace{2em}
        
        \large
        The Technical Challenge of AI Alignment
        
        \vspace{1em}
        
        Elisabetta Rocchetti
    \end{center}
\end{frame}

\end{document}
